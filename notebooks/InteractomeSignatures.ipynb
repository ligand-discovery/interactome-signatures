{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "In this notebook, we convert screening arrays into documents:\n",
    "* Document: fragment\n",
    "* Word: protein\n",
    "* Topic: group of proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert screening to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mduranfrigola/miniconda3/envs/topicmodel/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORT 64138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../results/topwiz_maxpxf100_minp4_maxp40/msigdb_gomf.joblib']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from socket import socket\n",
    "import blitzgsea as blitz\n",
    "import joblib\n",
    "import h5py\n",
    "import topicwizard\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from lol import LOL\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "MAX_PROTEIN_COUNTS = 40 # 40: 10% of fragments\n",
    "MIN_PROTEIN_COUNTS = 4 # 4: 1% of fragments\n",
    "MAX_PROTEINS_PER_FRAGMENTS = 100\n",
    "MIN_PROTEINS_PER_FRAGMENTS = 20\n",
    "TFIDF = False\n",
    "\n",
    "RESULTS_DIR = \"../results\"\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "\n",
    "with socket() as s:\n",
    "    s.bind(('',0))\n",
    "    PORT = s.getsockname()[1]\n",
    "print(\"PORT\", PORT)\n",
    "\n",
    "output_folder = \"topwiz_maxpxf{0}_minp{1}_maxp{2}\".format(MAX_PROTEINS_PER_FRAGMENTS, MIN_PROTEIN_COUNTS, MAX_PROTEIN_COUNTS)\n",
    "\n",
    "if os.path.exists(\"{0}/{1}\".format(RESULTS_DIR, output_folder)):\n",
    "    shutil.rmtree(\"{0}/{1}\".format(RESULTS_DIR, output_folder))\n",
    "os.mkdir(\"{0}/{1}\".format(RESULTS_DIR, output_folder))\n",
    "\n",
    "metadata = {\n",
    "    \"MAX_PROTEIN_COUNTS\": MAX_PROTEIN_COUNTS,\n",
    "    \"MIN_PROTEIN_COUNTS\": MIN_PROTEIN_COUNTS,\n",
    "    \"MAX_PROTEINS_PER_FRAGMENTS\": MAX_PROTEINS_PER_FRAGMENTS,\n",
    "    \"MIN_PROTEINS_PER_FRAGMENTS\": MIN_PROTEINS_PER_FRAGMENTS,\n",
    "    \"TFIDF\": TFIDF\n",
    "}\n",
    "with open(os.path.join(RESULTS_DIR, output_folder, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "def get_X_from_esm():\n",
    "    with h5py.File(\"../data/pid2seq_esm1b.h5\", \"r\") as f:\n",
    "        the_pids = [x.decode(\"utf-8\") for x in f[\"Keys\"][:]]\n",
    "        X = f[\"Values\"][:]\n",
    "        the_pids_idxs = dict((k,i) for i,k in enumerate(the_pids))\n",
    "    return X, the_pids, the_pids_idxs\n",
    "\n",
    "def get_X_from_bioteque(metapath_name=\"gene_has_cmp\"):\n",
    "    with h5py.File(\"../data/bioteque/{0}/GEN_emb.h5\".format(metapath_name), \"r\") as f:\n",
    "        X = f[\"m\"][:]\n",
    "    with open(\"../data/bioteque/{0}/GEN_ids.txt\".format(metapath_name), \"r\") as f:\n",
    "        the_pids = []\n",
    "        for l in f:\n",
    "            the_pids += [l.rstrip()]\n",
    "    the_pids_idxs = dict((k,i) for i,k in enumerate(the_pids))\n",
    "    return X, the_pids, the_pids_idxs\n",
    "\n",
    "def get_X_merged(n_components=300):\n",
    "    X_1, the_pids_1, the_pids_idxs_1 = get_X_from_bioteque(\"gene_has_cmp\")\n",
    "    X_2, the_pids_2, the_pids_idxs_2 = get_X_from_esm()\n",
    "    the_pids = sorted(set(the_pids_1).intersection(the_pids_2))\n",
    "    R = []\n",
    "    for pid in the_pids:\n",
    "        r = [x for x in X_1[the_pids_idxs_1[pid]]] + [x for x in X_2[the_pids_idxs_2[pid]]]\n",
    "        R += [r]\n",
    "    X = np.array(R)\n",
    "    X = PCA(n_components=n_components).fit_transform(X)\n",
    "    the_pids_idxs = dict((k,i) for i,k in enumerate(the_pids))\n",
    "    return X, the_pids, the_pids_idxs\n",
    "\n",
    "X_emb, emb_pids, emb_pids_idxs = get_X_merged()\n",
    "\n",
    "df = pd.read_csv(\"../data/cemm_primary_data.tsv\", sep=\"\\t\")\n",
    "df = df[df[\"UniqPeptides\"] >= 2]\n",
    "db = pd.read_csv(\"../data/cemm_primary_hit_data.tsv\", sep=\"\\t\")\n",
    "\n",
    "fids = sorted(set(df[\"FragID\"]))\n",
    "fid2smi = pd.read_csv(\"../data/fid2can_fff_all.tsv\", sep=\"\\t\")\n",
    "fid2smi = fid2smi[fid2smi[\"fid\"].isin(fids)].sort_values(\"fid\")\n",
    "fid2smi_dict = {}\n",
    "for r in fid2smi.values:\n",
    "    fid2smi_dict[r[0]] = r[1]\n",
    "\n",
    "pid2name = {}\n",
    "with open(\"../data/pid2name_primary_all.tsv\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for r in reader:\n",
    "        pid2name[r[0]] = r[1]\n",
    "\n",
    "protein_counts = collections.defaultdict(int)\n",
    "for r in db[\"UniProtID\"].tolist():\n",
    "    protein_counts[r] += 1\n",
    "protein_counts = sorted(protein_counts.items(), key=lambda x: -x[1])\n",
    "\n",
    "fragment_counts = collections.defaultdict(int)\n",
    "for r in db[\"FragID\"].tolist():\n",
    "    fragment_counts[r] += 1\n",
    "fragment_counts = sorted(fragment_counts.items(), key=lambda x: -x[1])\n",
    "\n",
    "df = pd.read_csv(\"../data/cemm_primary_data.tsv\", sep=\"\\t\")\n",
    "db = pd.read_csv(\"../data/cemm_primary_hit_data.tsv\", sep=\"\\t\")\n",
    "\n",
    "protein_counts = collections.defaultdict(int)\n",
    "for r in db[\"UniProtID\"].tolist():\n",
    "    protein_counts[r] += 1\n",
    "\n",
    "max_counts = MAX_PROTEIN_COUNTS\n",
    "pids = sorted(set(db[\"UniProtID\"]))\n",
    "pids = [x for x in pids if protein_counts[x] <= max_counts]\n",
    "min_counts = MIN_PROTEIN_COUNTS\n",
    "pids = [x for x in pids if protein_counts[x] >= min_counts]\n",
    "pids = sorted(set(pids).intersection(emb_pids))\n",
    "\n",
    "emb_idxs = []\n",
    "for pid in pids:\n",
    "    emb_idxs += [emb_pids_idxs[pid]]\n",
    "X_emb = X_emb[emb_idxs]\n",
    "\n",
    "pid2idx = dict((k,i) for i,k in enumerate(pids))\n",
    "pids_set = set(pids)\n",
    "\n",
    "fids = sorted(set(df[\"FragID\"]))\n",
    "fid2idx = dict((k,i) for i,k in enumerate(fids))\n",
    "\n",
    "fid2smi_ = pd.read_csv(\"../data/fid2can_fff_all.tsv\", delimiter=\"\\t\")\n",
    "fid2smi = {}\n",
    "for r in fid2smi_.values:\n",
    "    if r[0] in fid2idx:\n",
    "        fid2smi[r[0]] = r[1]\n",
    "\n",
    "Y_pro = np.full((len(fids), len(pids)), np.nan)\n",
    "\n",
    "for r in df[[\"FragID\", \"UniProtID\", \"UniqPeptides\", \"RankRelative\", \"Log2FC\", \"Log2FCMedian\", \"p\", \"pAdj\"]].values:\n",
    "    if r[1] not in pids_set:\n",
    "        continue\n",
    "    i = fid2idx[r[0]]\n",
    "    j = pid2idx[r[1]]\n",
    "    unique_peptides = float(r[2])\n",
    "    rank_relative = float(r[3])\n",
    "    log2fc = float(r[4])\n",
    "    log2fc_median = float(r[5])\n",
    "    p_value = float(r[6])\n",
    "    p_value_adj = float(r[7])\n",
    "    if unique_peptides < 2:\n",
    "        continue\n",
    "    if np.isnan(p_value):\n",
    "        continue\n",
    "    if np.isnan(log2fc):\n",
    "        continue\n",
    "    Y_pro[i,j] = log2fc\n",
    "\n",
    "R_pro = np.array(Y_pro)\n",
    "R_pro[np.isnan(R_pro)] = -999\n",
    "\n",
    "def read_annotations(annotations_file):\n",
    "    d = collections.defaultdict(list)\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for r in reader:\n",
    "            if r[0] not in pid2name:\n",
    "                continue\n",
    "            d[r[1]] += [pid2name[r[0]]]\n",
    "    d = dict((k, set(v)) for k,v in d.items() if len(v) >= 5)\n",
    "    return d\n",
    "\n",
    "cc_annotations = read_annotations(\"../data/msigdb_gocc.tsv\")\n",
    "mf_annotations = read_annotations(\"../data/msigdb_gomf.tsv\")\n",
    "\n",
    "joblib.dump(cc_annotations, os.path.join(RESULTS_DIR, output_folder, \"msigdb_gocc.joblib\"))\n",
    "joblib.dump(mf_annotations, os.path.join(RESULTS_DIR, output_folder, \"msigdb_gomf.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "gsea_prefiltering_file = os.path.join(RESULTS_DIR, \"gsea_prefiltering_minp{0}_maxp{1}.joblib\".format(MIN_PROTEIN_COUNTS, MAX_PROTEIN_COUNTS))\n",
    "\n",
    "if os.path.exists(gsea_prefiltering_file):\n",
    "    gsea_prefiltering_results = joblib.load(gsea_prefiltering_file)\n",
    "else:\n",
    "    gsea_prefiltering_results = []\n",
    "    for i in range(R_pro.shape[0]):\n",
    "        v = np.array(R_pro[i,:])\n",
    "        mask = v != -999\n",
    "        pids_ = [pids[i] for i in range(len(v)) if mask[i]]\n",
    "        v_ = PowerTransformer().fit_transform(v[mask].reshape(-1,1))[:,0]\n",
    "        genes_ = [pid2name[p] for p in pids_]\n",
    "        signature = pd.DataFrame({0: genes_, 1: v_})\n",
    "        cc_ = dict((k,list(v.intersection(genes_))) for k,v in cc_annotations.items())\n",
    "        mf_ = dict((k,list(v.intersection(genes_))) for k,v in mf_annotations.items())\n",
    "        cc_ = dict((k,v) for k,v in cc_.items() if len(v) >= 5)\n",
    "        mf_ = dict((k,v) for k,v in mf_.items() if len(v) >= 5)\n",
    "        result_cc = blitz.gsea(signature, cc_, anchors=4, permutations=2000, processes=4, signature_cache=True)\n",
    "        result_mf = blitz.gsea(signature, mf_, anchors=4, permutations=2000, processes=4, signature_cache=True)\n",
    "        gsea_prefiltering_results += [(fids[i], signature, result_cc, result_mf)]\n",
    "    joblib.dump(gsea_prefiltering_results, gsea_prefiltering_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_documents_list(min_proteins_per_fragment):\n",
    "    gsea_pval = 0.05\n",
    "    documents_list = []\n",
    "    cap = 250\n",
    "    hits = []\n",
    "    for fid in fids:\n",
    "        pids_ = db[db[\"FragID\"] == fid][\"UniProtID\"]\n",
    "        pids_ = set(pids_).intersection(pids)\n",
    "        genes_ = [pid2name[p] for p in pids_]\n",
    "        hits += [set(genes_)]\n",
    "    gid2idx = dict((pid2name[k],i) for i,k in enumerate(pids))\n",
    "    for i in range(R_pro.shape[0]):\n",
    "        gs = gsea_prefiltering_results[i]\n",
    "        cc = gs[2]\n",
    "        cc = cc[cc[\"pval\"] < gsea_pval]\n",
    "        mf = gs[3]\n",
    "        mf = mf[mf[\"pval\"] < gsea_pval]\n",
    "        cc_genes = [g for x in cc[\"leading_edge\"].tolist() for g in x.split(\",\")]\n",
    "        mf_genes = [g for x in mf[\"leading_edge\"].tolist() for g in x.split(\",\")]\n",
    "        le = list(cc_genes + mf_genes)\n",
    "        le = set([x for x in le if x != \"\"])\n",
    "        v = R_pro[i,:]\n",
    "        idxs = np.argsort(v)[::-1][:cap]\n",
    "        pids_ = [pids[i] for i in idxs]\n",
    "        genes_base = [pid2name[p] for p in pids_]\n",
    "        genes_ = [g for g in genes_base if g in le]\n",
    "        if len(genes_) > MAX_PROTEINS_PER_FRAGMENTS:\n",
    "            genes_ = genes_[:MAX_PROTEINS_PER_FRAGMENTS]\n",
    "        else:\n",
    "            if len(genes_) < min_proteins_per_fragment:\n",
    "                missing = min_proteins_per_fragment - len(genes_)\n",
    "                ht = hits[i]\n",
    "                exclusive_ht = ht.difference(le)\n",
    "                if len(exclusive_ht) >= missing:\n",
    "                    eh_list = list(exclusive_ht)\n",
    "                    le_list = list(le)\n",
    "                    eh_idxs = [gid2idx[k] for k in eh_list]\n",
    "                    le_idxs = [gid2idx[k] for k in le_list]\n",
    "                    nn = NearestNeighbors(n_neighbors=3)\n",
    "                    nn.fit(X_emb[le_idxs])\n",
    "                    I, D = nn.kneighbors(X_emb[eh_idxs])\n",
    "                    dists = np.mean(D, axis=1)\n",
    "                    idxs_ = np.argsort(dists)[:missing]\n",
    "                    sel_eh = set([eh_list[idx] for idx in idxs_])\n",
    "                    genes_ = [g for g in genes_base if g in le or g in sel_eh]\n",
    "                else:\n",
    "                    genes_ = [g for g in genes_base if g in le or g in ht]\n",
    "                    missing = min_proteins_per_fragment - len(genes_)\n",
    "                    eb_list = [g for g in genes_base if g not in genes_]\n",
    "                    eb_idxs = [gid2idx[k] for k in eb_list]\n",
    "                    le_list = list(le)\n",
    "                    le_idxs = [gid2idx[k] for k in le_list]\n",
    "                    nn = NearestNeighbors(n_neighbors=3)\n",
    "                    nn.fit(X_emb[le_idxs])\n",
    "                    I, D = nn.kneighbors(X_emb[eb_idxs])\n",
    "                    dists = np.mean(D, axis=1)\n",
    "                    idxs_ = np.argsort(dists)[:missing]\n",
    "                    sel_eb = set([eb_list[idx] for idx in idxs_])\n",
    "                    genes_ = [g for g in genes_base if g in le or g in ht or g in sel_eb]\n",
    "            else:\n",
    "                pass\n",
    "        documents_list += [genes_]\n",
    "    return documents_list\n",
    "\n",
    "def create_documents():\n",
    "    f0 = open(\"{1}/{0}/gene_docs.txt\".format(output_folder, RESULTS_DIR), \"w\")\n",
    "    f1 = open(\"{1}/{0}/fids.txt\".format(output_folder, RESULTS_DIR), \"w\")\n",
    "    for i, g in enumerate(documents_list):\n",
    "        f0.write(\" \".join(g) + \"\\n\")\n",
    "        f1.write(fids[i] + \"\\n\")\n",
    "    f0.close()\n",
    "    f1.close()\n",
    "\n",
    "documents_list = do_documents_list(MIN_PROTEINS_PER_FRAGMENTS)\n",
    "create_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy import spatial\n",
    "\n",
    "num_topic_trials = [10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "def tokenizer(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "if not TFIDF:\n",
    "    vectorizer = CountVectorizer(lowercase=False, tokenizer=tokenizer)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)\n",
    "\n",
    "def get_texts():\n",
    "    with open(\"{1}/{0}/gene_docs.txt\".format(output_folder, RESULTS_DIR), \"r\") as f:\n",
    "        texts = [x.rstrip() for x in f.readlines()]\n",
    "    return texts\n",
    "\n",
    "def get_doc_names():\n",
    "    with open(\"{1}/{0}/fids.txt\".format(output_folder, RESULTS_DIR), \"r\") as f:\n",
    "        doc_names = [x.rstrip() for x in f.readlines()]\n",
    "    return doc_names\n",
    "\n",
    "texts = get_texts()\n",
    "doc_names = get_doc_names()\n",
    " \n",
    "def fit_topic_pipeline(num_topics):\n",
    "    mdl = NMF(n_components=num_topics, max_iter=1000)\n",
    "    topic_pipeline = Pipeline(\n",
    "       [\n",
    "          (\"vec\", vectorizer),\n",
    "          (\"mdl\", mdl),\n",
    "       ]\n",
    "    )\n",
    "    texts = get_texts()\n",
    "    doc_names = get_doc_names()\n",
    "    topic_pipeline.fit(texts)\n",
    "    return topic_pipeline\n",
    "\n",
    "def calculate_coherence(term_rankings):\n",
    "    gid2idx = dict((pid2name[k],i) for i,k in enumerate(pids))\n",
    "    print(len(gid2idx))\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            t0 = pair[0]\n",
    "            t1 = pair[1]\n",
    "            x0 = X_emb[gid2idx[t0]]\n",
    "            x1 = X_emb[gid2idx[t1]]\n",
    "            pair_scores.append(1-spatial.distance.cosine(x0,x1))\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "def get_descriptor(all_terms, H, topic_index, top):\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( all_terms[term_index] )\n",
    "    return top_terms\n",
    "\n",
    "def screen_topics(topic_pipelines):\n",
    "    topic_models = []\n",
    "    for i, tp in enumerate(topic_pipelines):\n",
    "        k = num_topic_trials[i]\n",
    "        W = tp.transform(texts)\n",
    "        H = tp[\"mdl\"].components_\n",
    "        topic_models += [(k, W, H)]\n",
    "    k_values = []\n",
    "    coherences = []\n",
    "    terms = topicwizard.prepare.utils.get_vocab(tp[\"vec\"])\n",
    "    for (k,W,H) in topic_models:\n",
    "        term_rankings = []\n",
    "        for topic_index in range(k):\n",
    "            term_rankings.append(get_descriptor(terms, H, topic_index, 10))\n",
    "        k_values.append(k)\n",
    "        coherences.append(calculate_coherence(term_rankings))\n",
    "        print(\"K=%02d: Coherence=%.4f\" % (k, coherences[-1]))\n",
    "    return coherences\n",
    "\n",
    "topic_pipelines = [fit_topic_pipeline(n) for n in num_topic_trials]\n",
    "coherences = screen_topics(topic_pipelines)\n",
    "        \n",
    "NUM_TOPICS = 10\n",
    "topic_pipeline = fit_topic_pipeline(num_topics=NUM_TOPICS)\n",
    "vec = topic_pipeline[\"vec\"]\n",
    "mdl = topic_pipeline[\"mdl\"]\n",
    "    \n",
    "data_for_wizard = {\"corpus\": texts, \"pipeline\": topic_pipeline, \"document_names\": doc_names}\n",
    "\n",
    "joblib.dump(data_for_wizard, os.path.join(RESULTS_DIR, output_folder, \"data_for_wizard.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Wizard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import topicwizard\n",
    "\n",
    "data_for_wizard = joblib.load(os.path.join(RESULTS_DIR, output_folder, \"data_for_wizard.joblib\"))\n",
    "\n",
    "topicwizard.visualize(**data_for_wizard, port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8', 'Topic 9']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../results/topwiz_maxpxf100_minp4_maxp40/topic_data.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_names = [\"Topic {0}\".format(i) for i in range(NUM_TOPICS)]\n",
    "\n",
    "topic_data = {\n",
    "    \"document_names\": doc_names,\n",
    "    \"corpus\": texts,\n",
    "    \"vectorizer\": vec,\n",
    "    \"topic_model\": mdl,\n",
    "    \"topic_names\": topic_names\n",
    "}\n",
    "\n",
    "print(topic_names)\n",
    "\n",
    "joblib.dump(topic_data, os.path.join(RESULTS_DIR, output_folder, \"topic_data.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/topwiz_maxpxf100_minp4_maxp40/processed_topicwizard_data.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_data = joblib.load(os.path.join(RESULTS_DIR, output_folder, \"topic_data.joblib\"))\n",
    "\n",
    "document_names = topic_data[\"document_names\"]\n",
    "corpus = topic_data[\"corpus\"]\n",
    "vectorizer = topic_data[\"vectorizer\"]\n",
    "topic_model = topic_data[\"topic_model\"]\n",
    "topic_names = topic_data[\"topic_names\"]\n",
    "\n",
    "vocab = topicwizard.prepare.utils.get_vocab(vectorizer)\n",
    "document_term_matrix, document_topic_matrix, topic_term_matrix = topicwizard.prepare.utils.prepare_transformed_data(vectorizer, topic_model, corpus)\n",
    "topic_importances, term_importances, topic_term_importances = topicwizard.prepare.topics.topic_importances(topic_term_matrix, document_term_matrix, document_topic_matrix)\n",
    "\n",
    "word_pos = topicwizard.prepare.words.word_positions(topic_term_matrix)\n",
    "R = []\n",
    "for i,t in enumerate(vocab):\n",
    "    r = [t] + [term_importances[i]] + [word_pos[0][i], word_pos[1][i]] + [x for x in topic_term_importances[:,i]]\n",
    "    R += [r]\n",
    "\n",
    "d_ = pd.DataFrame(R, columns=[\"gene_name\", \"importance\"] + [\"proj_x\", \"proj_y\"] + topic_names)\n",
    "d_.to_csv(os.path.join(RESULTS_DIR, output_folder, \"ProteinHasTopic.csv\"), index=False)\n",
    "\n",
    "R = []\n",
    "for i,t in enumerate(topic_names):\n",
    "    R += [t] + [topic_importances[i]]\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "def has_tertiary_amine(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    patt = Chem.MolFromSmarts(\"[NX3]([CX4])([CX4])[CX4]\")\n",
    "    t = mol.HasSubstructMatch(patt)\n",
    "    if t:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "doc_pos = topicwizard.prepare.documents.document_positions(document_term_matrix)\n",
    "\n",
    "R = []\n",
    "for i in range(len(doc_names)):\n",
    "    fid_ = doc_names[i]\n",
    "    smi_ = fid2smi[fid_]\n",
    "    r = [fid_, smi_, has_tertiary_amine(smi_)] + [doc_pos[0][i], doc_pos[1][i]] + [x for x in document_topic_matrix[i]]\n",
    "    R += [r]\n",
    "\n",
    "d_ = pd.DataFrame(R, columns=[\"fid\", \"smiles\", \"tert_amine\"] + [\"proj_x\", \"proj_y\"] + topic_names)\n",
    "d_.to_csv(os.path.join(RESULTS_DIR, output_folder, \"FragmentHasTopic.csv\"), index=False)\n",
    "\n",
    "processed_topicwizard_data = {\n",
    "    \"vocab\": vocab,\n",
    "    \"document_term_matrix\": document_term_matrix,\n",
    "    \"document_topic_matrix\": document_topic_matrix,\n",
    "    \"topic_term_matrix\": topic_term_matrix,\n",
    "    \"topic_importances\": topic_importances,\n",
    "    \"term_importances\": term_importances,\n",
    "    \"topic_term_importances\": topic_term_importances,\n",
    "    \"document_topic_importances\": topicwizard.prepare.documents.document_topic_importances(document_topic_matrix)\n",
    "}\n",
    "\n",
    "joblib.dump(processed_topicwizard_data, os.path.join(RESULTS_DIR, output_folder, \"processed_topicwizard_data.joblib\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "W = document_topic_matrix\n",
    "\n",
    "sns.clustermap(pd.DataFrame(W, index=document_names, columns=topic_names), cmap=\"YlGnBu\")\n",
    "\n",
    "plt.savefig(os.path.join(RESULTS_DIR, output_folder, \"heatmap_docs_topic.png\"), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "H = topic_term_importances.T\n",
    "sns.clustermap(pd.DataFrame(H, index=vocab, columns=topic_names), cmap=\"YlGnBu\")\n",
    "\n",
    "plt.savefig(os.path.join(RESULTS_DIR, output_folder, \"heatmap_words_topic.png\"), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "mf_library = mf_annotations\n",
    "cc_library = cc_annotations\n",
    "\n",
    "genes = set(vocab)\n",
    "\n",
    "mf_sets = {}\n",
    "for k,v in mf_library.items():\n",
    "    g = genes.intersection(v)\n",
    "    if len(g) < 5:\n",
    "        continue\n",
    "    mf_sets[k] = list(g)\n",
    "\n",
    "cc_sets = {}\n",
    "for k,v in cc_library.items():\n",
    "    g = genes.intersection(v)\n",
    "    if len(g) < 5:\n",
    "        continue\n",
    "    cc_sets[k] = list(g)\n",
    "\n",
    "signatures = []\n",
    "for i, t in enumerate(topic_names):\n",
    "    v = topic_term_importances[i,:]\n",
    "    g = vocab\n",
    "    R = []\n",
    "    for j in range(len(v)):\n",
    "        R += [[g[j], v[j]]]\n",
    "    signature = pd.DataFrame(R)\n",
    "    signatures += [signature]\n",
    "\n",
    "gseas_mf = []\n",
    "for i, t in enumerate(topic_names):\n",
    "    result = blitz.gsea(signatures[i], mf_sets)\n",
    "    gseas_mf += [result]\n",
    "\n",
    "gseas_cc = []\n",
    "for i, t in enumerate(topic_names):\n",
    "    result = blitz.gsea(signatures[i], cc_sets)\n",
    "    gseas_cc += [result]\n",
    "\n",
    "gsea_results = {\n",
    "    \"topic_names\": topic_names,\n",
    "    \"signatures\": signatures,\n",
    "    \"cc_sets\": cc_sets,\n",
    "    \"mf_sets\": mf_sets,\n",
    "    \"gseas_cc\": gseas_cc,\n",
    "    \"gseas_mf\": gseas_mf\n",
    "}\n",
    "\n",
    "joblib.dump(gsea_results, os.path.join(RESULTS_DIR, output_folder, \"gsea_results.joblib\"))\n",
    "\n",
    "gsea_plots_folder = os.path.join(RESULTS_DIR, output_folder, \"gsea_plots\")\n",
    "if os.path.exists(gsea_plots_folder):\n",
    "    shutil.rmtree(gsea_plots_folder)\n",
    "os.mkdir(gsea_plots_folder)\n",
    "\n",
    "for i,t in enumerate(topic_names):\n",
    "    fig_table = blitz.plot.top_table(signatures[i], cc_sets, gseas_cc[i], n=15)\n",
    "    fig_table.savefig(os.path.join(gsea_plots_folder, \"{0}_cc.png\".format(t.replace(\" \", \"_\"))), bbox_inches='tight', transparent=False, facecolor=\"white\")\n",
    "    \n",
    "for i,t in enumerate(topic_names):\n",
    "    fig_table = blitz.plot.top_table(signatures[i], mf_sets, gseas_mf[i], n=15)\n",
    "    fig_table.savefig(os.path.join(gsea_plots_folder, \"{0}_mf.png\".format(t.replace(\" \", \"_\"))), bbox_inches='tight', transparent=False, facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import gseapy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def enrichr_analysis(num_proteins_per_topic):\n",
    "    enrs = []\n",
    "    for i in tqdm(range(NUM_TOPICS)):\n",
    "        d = topicwizard.prepare.topics.calculate_top_words(i, num_proteins_per_topic, 1, term_importances, topic_term_importances, vocab)\n",
    "        gene_list = list(d[\"word\"])\n",
    "        enr = gseapy.enrich(gene_list=gene_list, gene_sets=[cc_sets, mf_sets], background=[pid2name[p] for p in pids])\n",
    "        enrs += [enr]\n",
    "    joblib.dump(enrs, os.path.join(RESULTS_DIR, output_folder, \"enrichr_results_{0}.joblib\".format(num_proteins_per_topic)))\n",
    "    enrichr_results_folder = os.path.join(RESULTS_DIR, output_folder, \"enrichr_results_{0}\".format(num_proteins_per_topic))\n",
    "    if os.path.exists(enrichr_results_folder):\n",
    "        shutil.rmtree(enrichr_results_folder)\n",
    "    os.mkdir(enrichr_results_folder)\n",
    "    for i in range(len(topic_names)):\n",
    "        d_ = enrs[i].results\n",
    "        d_.to_csv(os.path.join(enrichr_results_folder, \"topic_{0}.csv\".format(i)), index=False)\n",
    "    return enrs\n",
    "\n",
    "enrs_0 = enrichr_analysis(5)\n",
    "enrs_1 = enrichr_analysis(10)\n",
    "enrs_2 = enrichr_analysis(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def plot_enrichr(enrs, num_proteins_per_topic):\n",
    "    enrichr_plots_folder = os.path.join(RESULTS_DIR, output_folder, \"enrichr_plots_{0}\".format(num_proteins_per_topic))\n",
    "    if os.path.exists(enrichr_plots_folder):\n",
    "        shutil.rmtree(enrichr_plots_folder)\n",
    "    os.mkdir(enrichr_plots_folder)\n",
    "\n",
    "    for i in range(len(enrs)):\n",
    "        try:\n",
    "            gseapy.barplot(enrs[i].results, group=\"Gene_set\", color=['darkred', 'darkblue'])\n",
    "            ax = plt.gca()\n",
    "            ax.set_facecolor(\"white\")\n",
    "            plt.savefig(os.path.join(enrichr_plots_folder, \"enrichr_topic_{0}.png\".format(i)), transparent=False, bbox_inches=\"tight\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "plot_enrichr(enrs_0, 5)\n",
    "plot_enrichr(enrs_1, 10)\n",
    "plot_enrichr(enrs_2, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fht = pd.read_csv(os.path.join(RESULTS_DIR, output_folder, \"FragmentHasTopic.csv\"))\n",
    "smiles_list = fht[\"smiles\"].tolist()\n",
    "W = np.array(fht[list(fht.columns)[5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_by_cutoff(W, cutoff=0.33, max_pos=0.1):\n",
    "    O = np.zeros(W.shape, dtype=int)\n",
    "    O[W > cutoff] = 1\n",
    "    max_pos = int(W.shape[0]*max_pos)\n",
    "    for i in range(O.shape[1]):\n",
    "        n = np.sum(O[:,i])\n",
    "        if n > max_pos:\n",
    "            idxs = np.argsort(W[:,i])[::-1][:max_pos]\n",
    "            O[:,i] = 0\n",
    "            O[idxs,i] = 1\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 412/412 [00:05<00:00, 78.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_0 [0.7461538461538462, 0.7692307692307693, 0.7461538461538462, 0.8974358974358975, 0.8410256410256409, 0.8871794871794872, 0.5871794871794872, 0.7717948717948718, 0.823076923076923, 0.4179487179487179]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_1 [0.8433333333333332, 0.7683333333333333, 0.8016666666666666, 0.7849999999999999, 0.75, 0.6933333333333334, 0.8283333333333333, 0.695, 0.71, 0.705]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_2 [0.9333333333333333, 0.9966666666666667, 0.9266666666666666, 0.9616666666666667, 0.9083333333333334, 0.9933333333333334, 0.9483333333333333, 0.9650000000000001, 0.99, 0.9583333333333334]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_3 [0.6699999999999999, 0.6016666666666667, 0.6983333333333333, 0.72, 0.7066666666666667, 0.5883333333333334, 0.665, 0.6416666666666666, 0.6216666666666667, 0.8083333333333333]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_4 [0.3564102564102564, 0.5743589743589743, 0.44358974358974357, 0.8179487179487179, 0.6794871794871795, 0.4307692307692308, 0.8102564102564103, 0.5897435897435898, 0.558974358974359, 0.6743589743589744]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_5 [0.6835443037974683, 0.6930379746835443, 0.8987341772151899, 0.6582278481012659, 0.8829113924050632, 0.8037974683544304, 0.860759493670886, 0.870253164556962, 0.5664556962025317, 0.7151898734177216]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_6 [0.841991341991342, 0.7034632034632035, 0.803030303030303, 0.7532467532467533, 0.7380952380952381, 0.8203463203463203, 0.8290043290043291, 0.8354978354978354, 0.7835497835497836, 0.9004329004329005]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_7 [0.43609022556390975, 0.5733082706766918, 0.6409774436090225, 0.47556390977443613, 0.5545112781954887, 0.5545112781954887, 0.5093984962406015, 0.6785714285714286, 0.5338345864661654, 0.6221804511278195]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_8 [0.5324675324675325, 0.738095238095238, 0.538961038961039, 0.7857142857142858, 0.47186147186147187, 0.6125541125541126, 0.6796536796536796, 0.6125541125541126, 0.7510822510822511, 0.7445887445887446]\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "signature_9 [0.9329004329004329, 0.9155844155844156, 0.8138528138528139, 0.8571428571428572, 0.8463203463203464, 0.8463203463203464, 0.7770562770562771, 0.8051948051948052, 0.751082251082251, 0.8917748917748918]\n"
     ]
    }
   ],
   "source": [
    "from fragmentembedding import FragmentEmbedder\n",
    "from miniautoml.binary_classifier import train_binary_classifier\n",
    "\n",
    "Y = binarize_by_cutoff(W)\n",
    "R = []\n",
    "for i in range(len(fids)):\n",
    "    r = [fids[i], fid2smi[fids[i]]] + [y_ for y_ in Y[i,:]]\n",
    "    R += [r]\n",
    "ds = pd.DataFrame(R, columns=[\"fid\", \"smiles\"]+[\"signature_{0}\".format(i) for i in range(NUM_TOPICS)])\n",
    "ds.to_csv(os.path.join(RESULTS_DIR, output_folder, \"fragment_signatures.csv\"), index=False)\n",
    "\n",
    "smiles_list = ds[\"smiles\"].tolist()\n",
    "\n",
    "X = FragmentEmbedder().transform(smiles_list)\n",
    "\n",
    "mdl_folder = os.path.join(RESULTS_DIR, output_folder, \"modeling_validations\")\n",
    "if os.path.exists(mdl_folder):\n",
    "    shutil.rmtree(mdl_folder)\n",
    "os.mkdir(mdl_folder)\n",
    "\n",
    "validation_metrics_all_topics = []\n",
    "y_columns = list(ds.columns[2:])\n",
    "for yc in y_columns:\n",
    "    y = ds[yc].tolist()\n",
    "    mdl = train_binary_classifier(X, y, n_splits=10)\n",
    "    print(yc, mdl.validation_metrics[\"aucs\"])\n",
    "    joblib.dump(mdl, os.path.join(mdl_folder, yc+\".joblib\"))\n",
    "    validation_metrics_all_topics += [mdl.validation_metrics]\n",
    "\n",
    "joblib.dump(validation_metrics_all_topics, os.path.join(mdl_folder, \"validation_metrics.joblib\"))\n",
    "\n",
    "np.save(os.path.join(RESULTS_DIR, output_folder, \"modeling_validations\", \"X.npy\"), X)\n",
    "np.save(os.path.join(RESULTS_DIR, output_folder, \"modeling_validations\", \"Y.npy\"), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mduranfrigola/miniconda3/envs/topicmodel/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator VarianceThreshold from version 0.23.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "/Users/mduranfrigola/miniconda3/envs/topicmodel/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator VarianceThreshold from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "/Users/mduranfrigola/miniconda3/envs/topicmodel/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator KBinsDiscretizer from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:13<00:00, 78.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:13<00:00, 75.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:13<00:00, 76.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:13<00:00, 75.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:13<00:00, 74.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 560/560 [00:07<00:00, 76.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_0\n",
      "signature_1\n",
      "signature_2\n",
      "signature_3\n",
      "signature_4\n",
      "signature_5\n",
      "signature_6\n",
      "signature_7\n",
      "signature_8\n",
      "signature_9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/enamine_stock.csv\")\n",
    "ds = pd.read_csv(os.path.join(RESULTS_DIR, output_folder, \"fragment_signatures.csv\"))\n",
    "headers = list(ds.columns)[2:]\n",
    "\n",
    "smiles_list = df[\"smiles\"].tolist()\n",
    "identifiers = df[\"catalog_id\"].tolist()\n",
    "X = FragmentEmbedder().transform(smiles_list)\n",
    "\n",
    "R = []\n",
    "for h in headers:\n",
    "    print(h)\n",
    "    mdl = joblib.load(os.path.join(mdl_folder, h+\".joblib\"))\n",
    "    y_hat = list(mdl.predict(X))\n",
    "    R += [y_hat]\n",
    "Y_hat = np.array(R).T\n",
    "\n",
    "R = []\n",
    "for i in range(Y_hat.shape[0]):\n",
    "    r = [identifiers[i], smiles_list[i]] + [y_ for y_ in Y_hat[i,:]]\n",
    "    R += [r]\n",
    "\n",
    "df = pd.DataFrame(R, columns = [\"catalog_id\", \"smiles\"] + headers)\n",
    "df.to_csv(os.path.join(mdl_folder, \"enamine_predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import stylia\n",
    "fig, axs = stylia.create_figure(1,1)\n",
    "ax = axs.next()\n",
    "for i, vd in enumerate(validation_metrics_all_topics):\n",
    "    vals = np.array(vd[\"aucs\"])\n",
    "    noise = np.random.normal(0, 0.1, size=len(vals))\n",
    "    ax.scatter(noise+i, vals)\n",
    "ax.set_xticks([i for i in range(NUM_TOPICS)])\n",
    "stylia.label(ax, title=\"ROC validation\", xlabel=\"Topics\", ylabel=\"AUROC\")\n",
    "    \n",
    "stylia.save_figure(os.path.join(RESULTS_DIR, output_folder, \"modeling_validations\", \"aucs.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = []\n",
    "for i, vd in enumerate(validation_metrics_all_topics):\n",
    "    r = [\"Topic {0}\".format(i), vd[\"positives\"], np.mean(vd[\"aucs\"]), np.std(vd[\"aucs\"]), np.min(vd[\"aucs\"]), np.max(vd[\"aucs\"])]\n",
    "    R += [r]\n",
    "pd.DataFrame(R, columns=[\"topic\", \"positives\", \"auroc_mean\", \"auroc_std\", \"auroc_min\", \"auroc_max\"]).to_csv(os.path.join(mdl_folder, \"validation_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichment figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster import hierarchy\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import stylia\n",
    "from stylia.colors import ContinuousColorMap\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "data_dir = os.path.join(RESULTS_DIR, output_folder)\n",
    "\n",
    "top_proteins = [5, 10, 25]\n",
    "\n",
    "gs_inds = {\n",
    "    \"gs_ind_0\": \"CC\",\n",
    "    \"gs_ind_1\": \"MF\"\n",
    "}\n",
    "\n",
    "R = []\n",
    "for t in top_proteins:\n",
    "    for i in range(NUM_TOPICS):\n",
    "        s = \"topic_{0}.csv\".format(i)\n",
    "        file_name = os.path.join(data_dir, \"enrichr_results_{0}\".format(t), \"topic_{0}.csv\".format(i))\n",
    "        df = pd.read_csv(file_name)\n",
    "        df = df[df[\"Adjusted P-value\"] < 0.01]\n",
    "        for v in df.values:\n",
    "            gs = gs_inds[v[0]]\n",
    "            term = v[1]\n",
    "            over = int(v[2].split(\"/\")[0])\n",
    "            total = int(v[2].split(\"/\")[1])\n",
    "            pval = -np.log10(v[3])\n",
    "            adj_pval = -np.log10(v[4])\n",
    "            odds = v[5]\n",
    "            genes = v[6].split(\";\")\n",
    "            topic = \"Topic {0}\".format(i)\n",
    "            R += [[t, gs, term, topic, over, total, pval, adj_pval, odds, genes]]\n",
    "            \n",
    "unique_terms = sorted(set([(r[1], r[2]) for r in R]))\n",
    "ut_idxs = dict((k,i) for i,k in enumerate(unique_terms))\n",
    "topic_names = [\"Topic {0}\".format(i) for i in range(NUM_TOPICS)]\n",
    "topic_tops = [(tn, t) for tn in topic_names for t in top_proteins]\n",
    "tt_idxs = dict((k,i) for i,k in enumerate(topic_tops))\n",
    "\n",
    "M = np.zeros((len(unique_terms), len(topic_tops)))\n",
    "for r in R:\n",
    "    i = ut_idxs[(r[1], r[2])]\n",
    "    j = tt_idxs[(r[3], r[0])]\n",
    "    v = r[7]\n",
    "    M[i,j] = v\n",
    "    \n",
    "M[M > 10] = 10\n",
    "M[M < 2] = 0\n",
    "\n",
    "Z = hierarchy.ward(M)\n",
    "hierarchy.leaves_list(Z)\n",
    "clustered_idxs = hierarchy.leaves_list(hierarchy.optimal_leaf_ordering(Z, M))\n",
    "\n",
    "unique_terms = [unique_terms[idx] for idx in clustered_idxs]\n",
    "M = M[clustered_idxs]\n",
    "\n",
    "term_genes = {}\n",
    "for ut in unique_terms:\n",
    "    d = collections.defaultdict(int)\n",
    "    for r in R:\n",
    "        k = (r[1], r[2])\n",
    "        if k == ut:\n",
    "            for g in r[-1]:\n",
    "                d[g] += 1\n",
    "    genes = [x[0] for x in sorted(d.items(), key=lambda x: -x[1])]\n",
    "    term_genes[ut] = genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "cmap = ContinuousColorMap()\n",
    "cmap.fit([i for i in range(len(topic_names))])\n",
    "topic_colors = cmap.transform([i for i in range(len(topic_names))])\n",
    "topic_color_dict = dict((topic_names[i], c) for i,c in enumerate(topic_colors))\n",
    "\n",
    "fig, axs = stylia.create_figure(1,1, width=15, height=20)\n",
    "ax = axs.next()\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "ss = []\n",
    "cs = []\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        v = M[i,j]\n",
    "        xs += [j]\n",
    "        ys += [i]\n",
    "        ss += [v*10]\n",
    "        cs += [topic_color_dict[topic_tops[j][0]]]\n",
    "ax.scatter(xs, ys, s=ss, color=cs)\n",
    "xticks = []\n",
    "xticklabels = []\n",
    "for i, t in enumerate(topic_tops):\n",
    "    xticks += [i]\n",
    "    if t[1] == top_proteins[1]:\n",
    "        xticklabels += [t[0].split(\" \")[1]]\n",
    "    else:\n",
    "        xticklabels += [\"\"]\n",
    "yticks = [i for i in range(M.shape[0])]\n",
    "yticklabels = [\"{0} [{1}]\".format(t[1], t[0]) for t in unique_terms]\n",
    "yticklabels_right = [\" \".join(term_genes[t][:10]) for t in unique_terms]\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_ylim(-1, M.shape[0])\n",
    "ax.set_xlim(-1, M.shape[1])\n",
    "ax.set_xticklabels(xticklabels)\n",
    "ax.set_yticklabels(yticklabels)\n",
    "ax_right = ax.twinx()\n",
    "ax_right.set_yticks(yticks)\n",
    "ax_right.set_yticklabels(yticklabels_right)\n",
    "ax_right.set_ylim(-1, M.shape[0])\n",
    "ax_right.grid(visible=False)\n",
    "\n",
    "stylia.label(ax, title=\"Enrichment analysis of topics\", xlabel = \"Topics\", ylabel = \"\")\n",
    "plt.tight_layout()\n",
    "\n",
    "stylia.save_figure(os.path.join(data_dir, \"enrichr_matrix.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggest topic names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_scores = collections.defaultdict(list)\n",
    "\n",
    "for i in range(NUM_TOPICS):\n",
    "    for n in [5, 10, 25]:\n",
    "        enr = pd.read_csv(os.path.join(RESULTS_DIR, output_folder, \"enrichr_results_{0}\".format(n), \"topic_{0}.csv\".format(i)))\n",
    "        for r in enr.values:\n",
    "            k = (i, r[0], r[1])\n",
    "            pval = r[3]\n",
    "            if pval > 0.05:\n",
    "                continue\n",
    "            v = -np.log10(pval)\n",
    "            #v = r[5]\n",
    "            enr_scores[k] += [v]\n",
    "\n",
    "agg_enr_scores = collections.defaultdict(list)\n",
    "\n",
    "for k,v in enr_scores.items():\n",
    "    agg_enr_scores[k[0]] += [(k[1], k[2], np.max(v))]\n",
    "\n",
    "topic_annotations = {}\n",
    "for k,v in agg_enr_scores.items():\n",
    "    v = sorted(v, key=lambda x: -x[2])\n",
    "    topic_annotations[k] = v[:5]\n",
    "    \n",
    "R = []\n",
    "for k,v in topic_annotations.items():\n",
    "    for x in v:\n",
    "        R += [[k, x[0], x[1], x[2]]]\n",
    "        \n",
    "enr = pd.DataFrame(R, columns=[\"topic\", \"go_cat\", \"term\", \"logpval_sum\"])\n",
    "\n",
    "enr.to_csv(os.path.join(RESULTS_DIR, output_folder, \"enrichr_summary_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac1afd3f21655ecdb48151aea2826cd5e4650b7d9a96e2157b180e35698f2976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
